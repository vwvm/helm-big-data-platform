apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-thrift-config
  labels:
    app: spark
    component: thrift-server-config
data:
  # ============ 1. Spark 环境配置文件 (spark-env.sh) ============
  spark-env.sh: |
    #!/bin/bash
    # Spark Environment Configuration for Thrift Server
    
    # Java Configuration
    export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
    
    # Thrift Server specific
    export SPARK_DRIVER_MEMORY=512m
    export SPARK_EXECUTOR_MEMORY=1000m
    export SPARK_WORKER_CORES=2
    
    # HDFS Configuration - 关键配置！
    export SPARK_HADOOP_FS_DEFAULTFS=hdfs://hadoop-namenode:8020
    
    # Hive Metastore
    export SPARK_HIVE_METASTORE_URIS=thrift://hive-metastore-service:9083
    
    # Logging
    export SPARK_LOG_DIR=/opt/spark/logs
    mkdir -p ${SPARK_LOG_DIR}
    
    # Hadoop config if exists
    if [ -d "/etc/hadoop/conf" ]; then
      export HADOOP_CONF_DIR=/etc/hadoop/conf
    fi

  # ============ 2. 启动脚本 (无sleep，直接前台运行) ============
  start-thrift-server.sh: |
    #!/bin/bash
    set -e
    
    echo "Starting Spark Thrift Server..."
    
    # 加载配置
    if [ -f "/opt/spark/conf/spark-env.sh" ]; then
      source /opt/spark/conf/spark-env.sh
    fi
    
    # 构建Hadoop配置参数
    HADOOP_CONF_ARGS=""
    if [ -n "${SPARK_HADOOP_FS_DEFAULTFS}" ]; then
      HADOOP_CONF_ARGS="--conf spark.hadoop.fs.defaultFS=${SPARK_HADOOP_FS_DEFAULTFS}"
      HADOOP_CONF_ARGS="${HADOOP_CONF_ARGS} --conf spark.sql.warehouse.dir=${SPARK_HADOOP_FS_DEFAULTFS}/user/hive/warehouse"
    fi
    
    if [ -n "${SPARK_HIVE_METASTORE_URIS}" ]; then
      HADOOP_CONF_ARGS="${HADOOP_CONF_ARGS} --conf spark.hadoop.hive.metastore.uris=${SPARK_HIVE_METASTORE_URIS}"
    fi
    
    # 直接运行thriftserver（前台进程）
    exec /opt/spark/sbin/start-thriftserver.sh \
      --master spark://spark-master:7077 \
      --conf spark.driver.memory=${SPARK_DRIVER_MEMORY:-512m} \
      --conf spark.driver.cores=1 \
      --conf spark.executor.memory=${SPARK_EXECUTOR_MEMORY:-1000m} \
      --conf spark.cores.max=${SPARK_WORKER_CORES:-2} \
      --conf spark.driver.port=30000 \
      --conf spark.blockManager.port=30001 \
      --conf spark.driver.bindAddress=0.0.0.0 \
      --conf spark.driver.host=$(hostname -i) \
      --conf spark.sql.catalogImplementation=hive \
      ${HADOOP_CONF_ARGS} \
      --name "Spark-Thrift-Server" \
      --hiveconf hive.server2.thrift.port=10000 \
      --hiveconf hive.server2.thrift.bind.host=0.0.0.0
    # exec确保thriftserver成为容器主进程，容器生命周期与thriftserver一致
  