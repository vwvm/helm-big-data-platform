apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: spark-thrift-server
spec:
  serviceName: spark-thrift-server
  replicas: 1
  selector:
    matchLabels:
      app: spark
      component: thrift-server
  template:
    metadata:
      labels:
        app: spark
        component: thrift-server
    spec:
      containers:
        - name: thrift-server
          image: docker.1ms.run/spark:4.0.1-scala2.13-java17-python3-r-ubuntu
          command: [ "/bin/bash", "-c" ]
          args:
            - |
              # 最简单的启动方式
              /opt/spark/sbin/start-thriftserver.sh \
              --master spark://spark-master:7077 \
              --conf spark.driver.memory=512m \
              --conf spark.driver.cores=1 \
              --conf spark.executor-memory=1000m \
              --conf spark.total-executor-cores=2 \
              --conf spark.driver.port=30000 \
              --conf spark.blockManager.port=30001 \
              --conf spark.driver.bindAddress=0.0.0.0 \
              --jars /opt/spark/jars/user/hadoop-lzo-0.4.20.jar \
              --conf spark.driver.extraClassPath=/opt/spark/jars/user/hadoop-lzo-0.4.20.jar \
              --conf spark.executor.extraClassPath=/opt/spark/jars/user/hadoop-lzo-0.4.20.jar \
              --conf spark.authenticate=false \
              --conf spark.authenticate.enableSaslEncryption=false \
              --conf spark.network.crypto.enabled=false \
              --conf spark.ssl.enabled=false \
              --conf spark.ui.enabled=false \
              --conf spark.sql.catalogImplementation=hive \
              --conf spark.hadoop.hive.metastore.uris=thrift://hive-metastore-service:9083 \
              --conf spark.hadoop.fs.defaultFS=hdfs://hadoop-namenode:8020
              # 保持容器运行
              tail -f /dev/null
          ports:
            - containerPort: 10000
            - containerPort: 30000  # 固定Driver端口
            - containerPort: 30001  # 固定BlockManager端口
            - containerPort: 4040
          resources:
            requests:
              memory: "1000Mi"
              cpu: "500m"
            limits:
              memory: "2Gi"
              cpu: "1000m"
          volumeMounts:
            - name: nfs-jars
              mountPath: /opt/spark/jars/user
              subPath: lib
            - name: hive-config
              mountPath: /opt/spark/conf/hive-site.xml
              subPath: hive-site.xml
              readOnly: true
            - name: hadoop-config
              mountPath: /opt/spark/conf/core-site.xml
              subPath: core-site.xml
              readOnly: true
            - name: spark-thrift-config
              mountPath: /opt/spark/conf/spark-defaults.conf
              subPath: spark-defaults.conf
      volumes:
        - name: nfs-jars
          persistentVolumeClaim:
            claimName: nfs-pvc
        - name: hive-config
          configMap:
            name: hive-config
        - name: hadoop-config
          configMap:
            name: hadoop-config
        - name: spark-thrift-config
          configMap:
            name: spark-thrift-config